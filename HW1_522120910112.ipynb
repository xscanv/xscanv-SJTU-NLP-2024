{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a31d058a6316b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业一：实现HMM中文分词和BPE英文分词\n",
    "姓名：\n",
    "\n",
    "学号："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## 任务一：HMM模型用于中文分词\n",
    "\n",
    "任务一评分标准：\n",
    "1. 共有8处TODO需要填写，每个TODO计1-2分，共9分，预计代码量30行。\n",
    "2. 允许自行修改、编写代码完成。对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分。\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "注：本任务仅在短句子上进行效果测试，因此对概率的计算可直接进行连乘。在实践中，常先对概率取对数，将连乘变为加法来计算，以避免出现数值溢出的情况。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc4dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "导入HMM参数，初始化所需的起始概率矩阵、转移概率矩阵、发射概率矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d25beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# 非断字（B）为第0行，断字（I）为第1行\n",
    "# 发射概率矩阵中，词典大小为65536，以汉字的Unicode码点（一个整数值）作为行索引\n",
    "start_probability = hmm_parameters[\"start_prob\"]  # shape(2,)\n",
    "trans_matrix = hmm_parameters[\"trans_mat\"]  # shape(2, 2)\n",
    "emission_matrix = hmm_parameters[\"emission_mat\"]  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "定义待处理的句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87219e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 将your_name中的xxx替换为你的姓名【1分】\n",
    "your_name = \"萧明远\"\n",
    "input_sentence = f\"{your_name}是一名优秀的学生\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "实现Viterbi算法，并以此进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1adac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Viterbi算法进行中文分词。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        str - 中文分词的结果\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种标注（B/I）的最大概率值\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # `path`用来储存最大概率对应的上步B/I选择\n",
    "    #  例如 path[1][7] == 1 意味着第8个（从1开始计数）字符标注I对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[0][5] == 1 意味着第6个字符标注B对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[1][1] == 0 意味着第2个字符标注I对应的最大概率，其前一步的隐状态为0（B）\n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "\n",
    "    #  TODO: 第一个位置的最大概率值计算【1分】\n",
    "    dp[0, 0] = start_prob[0] * emission_mat[0, sent_ord[0]]\n",
    "    dp[1, 0] = start_prob[1] * emission_mat[1, sent_ord[0]]\n",
    "\n",
    "    #  TODO: 其余位置的最大概率值计算（填充dp和path矩阵）【2分】\n",
    "    for t in range(1, len(sent_ord)):\n",
    "        for s in range(2):\n",
    "            prob0 = dp[0, t-1] * trans_mat[0, s] * emission_mat[s, sent_ord[t]]\n",
    "            prob1 = dp[1, t-1] * trans_mat[1, s] * emission_mat[s, sent_ord[t]]\n",
    "            if prob0 > prob1:\n",
    "                dp[s, t] = prob0\n",
    "                path[s, t] = 0\n",
    "            else:\n",
    "                dp[s, t] = prob1\n",
    "                path[s, t] = 1\n",
    "\n",
    "    #  `labels`用来储存每个位置最有可能的隐状态\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "\n",
    "    #  TODO: 计算labels每个位置上的值（填充labels矩阵）【1分】\n",
    "    labels[-1] = np.argmax(dp[:, -1])\n",
    "    for t in range(len(sent_ord) - 2, -1, -1):\n",
    "        labels[t] = path[labels[t + 1], t + 1]\n",
    "\n",
    "    #  根据lalels生成切分好的字符串\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d762d3",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d795414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi算法分词结果： 萧明/远是/一名/优秀/的/学生/\n"
     ]
    }
   ],
   "source": [
    "print(\"Viterbi算法分词结果：\", viterbi(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "实现前向算法，计算该句子的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_forward(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    前向算法，计算输入中文句子的概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 初始位置概率的计算【1分】\n",
    "    dp[0, 0] = start_prob[0] * emission_mat[0, sent_ord[0]]\n",
    "    dp[1, 0] = start_prob[1] * emission_mat[1, sent_ord[0]]\n",
    "\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【1分】\n",
    "    for t in range(1, len(sent_ord)):\n",
    "        dp[0, t] = (dp[0, t-1] * trans_mat[0, 0] + dp[1, t-1] * trans_mat[1, 0]) * emission_mat[0, sent_ord[t]]\n",
    "        dp[1, t] = (dp[0, t-1] * trans_mat[0, 1] + dp[1, t-1] * trans_mat[1, 1]) * emission_mat[1, sent_ord[t]]\n",
    "\n",
    "    return sum([dp[i][len(sent_ord) - 1] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "实现后向算法，计算该句子的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e898306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_backward(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    后向算法，计算输入中文句子的概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，从结尾到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 终末位置概率的初始化【1分】\n",
    "    dp[0, -1] = 1.0\n",
    "    dp[1, -1] = 1.0\n",
    "\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【1分】\n",
    "    for t in range(len(sent_ord) - 2, -1, -1):\n",
    "        dp[0, t] = dp[0, t + 1] * trans_mat[0, 0] * emission_mat[0, sent_ord[t + 1]] + \\\n",
    "                   dp[1, t + 1] * trans_mat[0, 1] * emission_mat[1, sent_ord[t + 1]]\n",
    "        dp[1, t] = dp[0, t + 1] * trans_mat[1, 0] * emission_mat[0, sent_ord[t + 1]] + \\\n",
    "                   dp[1, t + 1] * trans_mat[1, 1] * emission_mat[1, sent_ord[t + 1]]\n",
    "\n",
    "    return sum([dp[i][0] * start_prob[i] * emission_mat[i][sent_ord[0]] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed6cf5",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向算法概率： 9.014396408449741e-34\n",
      "后向算法概率： 9.014396408449741e-34\n"
     ]
    }
   ],
   "source": [
    "print(\"前向算法概率：\", compute_prob_by_forward(input_sentence, start_probability, trans_matrix, emission_matrix))\n",
    "print(\"后向算法概率：\", compute_prob_by_backward(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f6a1a465dd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 实验总结\n",
    "> \n",
    "在本次实验中，我们分别使用了前向算法（Forward Algorithm）和后向算法（Backward Algorithm）来计算中文句子的概率，并使用了Viterbi算法（Viterbi Algorithm）进行分词。\\\n",
    "其中，前向算法通过从句子的开头开始逐步计算每个位置的概率，最终累积到整个句子的概率。\\\n",
    "后向算法则从句子的结尾开始逐步计算每个位置的概率，最终累积到整个句子的概率。\\\n",
    "Viterbi算法则通过动态规划的思想，找到最优路径，从而实现分词。\\\n",
    "本次实验深入理解了HMM在中文分词中的应用。前向和后向算法为HMM提供了概率计算工具，而Viterbi算法在解码时提供了最优路径的确定。通过这些算法，我们能够准确地获取句子的分词概率和最优分词路径。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## 任务二：BPE算法用于英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "任务二评分标准：\n",
    "\n",
    "1. 共有6处TODO需要填写，每个TODO计1-2分，共9分，预计代码量50行。\n",
    "2. 允许自行修改、编写代码完成。对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分。\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02463b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5dbb9",
   "metadata": {},
   "source": [
    "构建空格分词器，将语料中的句子以空格切分成单词，然后将单词拆分成字母加`</w>`的形式。例如`apple`将变为`a p p l e </w>`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c3667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "\n",
    "def white_space_tokenize(corpus: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    先正则化（字母转小写、数字转为N、除去标点符号），然后以空格分词语料中的句子，例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到 `[[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]`\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - 二维List，内部的List由每个句子的单词str构成\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(filter(lambda token: len(token) > 0, _splitor_pattern.split(_digit_pattern.sub(\"N\", sentence.lower())))) for sentence in corpus]\n",
    "\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "编写相应函数构建BPE算法需要用到的初始状态词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf823e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bpe_vocab(corpus: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    将语料进行空格分词处理后，将单词每个字母以空格隔开、结尾加上</w>后，构建带频数的字典。\n",
    "    \n",
    "    例如：\n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，\n",
    "    返回结果为:\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        '1 0 </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bpe_vocab = {}\n",
    "\n",
    "    for sentence in corpus:\n",
    "        # 去除标点符号并转换为小写\n",
    "        sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence).lower()\n",
    "        # 分词\n",
    "        words = sentence.split()\n",
    "\n",
    "        for word in words:\n",
    "            # 将每个单词的字母分开并在末尾加上 </w>\n",
    "            bpe_word = ' '.join(list(word)) + ' </w>'\n",
    "            # 构建频数词典\n",
    "            if bpe_word in bpe_vocab:\n",
    "                bpe_vocab[bpe_word] += 1\n",
    "            else:\n",
    "                bpe_vocab[bpe_word] = 1\n",
    "\n",
    "    return bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a85d0e",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1693d458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i </w>': 2, 'a m </w>': 1, 'h a p p y </w>': 1, 'h a v e </w>': 1, '1 0 </w>': 1, 'a p p l e s </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"I am happy.\", \"I have 10 apples!\"]\n",
    "print(build_bpe_vocab(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "编写所需的其他函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_freq(bpe_vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    统计\"单词分词状态->频数\"的词典中，各bigram的频次（假设该词典中，各个unigram以空格间隔），例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple[str, str], int] - \"bigram->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】\n",
    "    for word, freq in bpe_vocab.items():\n",
    "        # 将单词按照空格分隔为各个 unigram\n",
    "        tokens = word.split()\n",
    "        \n",
    "        # 统计相邻 unigram 组成的 bigram\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = (tokens[i], tokens[i + 1])\n",
    "            if bigram in bigram_freq:\n",
    "                bigram_freq[bigram] += freq\n",
    "            else:\n",
    "                bigram_freq[bigram] = freq\n",
    "\n",
    "    return bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d2e9f",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2151666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('i', '</w>'): 2, ('a', 'm'): 1, ('m', '</w>'): 1, ('h', 'a'): 2, ('a', 'p'): 2, ('p', 'p'): 2, ('p', 'y'): 1, ('y', '</w>'): 1, ('a', 'v'): 1, ('v', 'e'): 1, ('e', '</w>'): 1, ('N', '</w>'): 1, ('p', 'l'): 1, ('l', 'e'): 1, ('e', 's'): 1, ('s', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "print(get_bigram_freq(bpe_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba426043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram: Tuple[str, str], old_bpe_vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    在\"单词分词状态->频数\"的词典中，合并指定的bigram（即去掉对应的相邻unigram之间的空格），最后返回新的词典，例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bigram = ('i', '</w>'), old_bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        bigram: Tuple[str, str] - 待合并的bigram\n",
    "        old_bpe_vocab: Dict[str, int] - 初始\"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - 合并后的\"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    new_bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】\n",
    "    bigram_str = \" \".join(bigram)  # 将 bigram 组合成一个字符串，例如 'i </w>'\n",
    "    merged_bigram = \"\".join(bigram)  # 合并 bigram，例如 'i</w>'\n",
    "\n",
    "    for word, freq in old_bpe_vocab.items():\n",
    "        # 替换 bigram 为合并形式\n",
    "        new_word = word.replace(bigram_str, merged_bigram)\n",
    "        new_bpe_vocab[new_word] = freq\n",
    "\n",
    "    return new_bpe_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1a959",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9fb269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i</w>': 2, 'a m </w>': 1, 'h a p p y </w>': 1, 'h a v e </w>': 1, 'N </w>': 1, 'a p p l e s </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "bigram = ('i', '</w>')\n",
    "old_bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "result = refresh_bpe_vocab_by_merging_bigram(bigram, old_bpe_vocab)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "992438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_tokens(bpe_vocab: Dict[str, int]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    根据\"单词分词状态->频数\"的词典，返回所得到的BPE分词列表，并将该列表按照分词长度降序排序返回，例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('a', 2),\n",
    "        ('m', 1),\n",
    "        ('</w>', 5),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('e', 2),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        List[Tuple[str, int]] - BPE分词和对应频数组成的List\n",
    "    \"\"\"\n",
    "\n",
    "    bpe_tokens = []\n",
    "    # TODO: 完成函数体【2分】\n",
    "    from collections import defaultdict\n",
    "    token_freq = defaultdict(int)\n",
    "\n",
    "    # 遍历词典，计算各个 BPE token 的频数\n",
    "    for word, freq in bpe_vocab.items():\n",
    "        tokens = word.split()  # 分隔成单个 token\n",
    "        for token in tokens:\n",
    "            token_freq[token] += freq\n",
    "\n",
    "    # 将 token 和频数转为列表，并按 token 长度降序排序\n",
    "    bpe_tokens = sorted(token_freq.items(), key=lambda x: (-len(x[0]), x[0]))\n",
    "\n",
    "    return bpe_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134fd89",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3032b76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i</w>', 2), ('</w>', 5), ('ha', 2), ('pp', 2), ('N', 1), ('a', 2), ('e', 2), ('l', 1), ('m', 1), ('s', 1), ('v', 1), ('y', 1)]\n"
     ]
    }
   ],
   "source": [
    "bpe_vocab = {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "result = get_bpe_tokens(bpe_vocab)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c56995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def print_bpe_tokenize(word: str, bpe_tokens: List[Tuple[str, int]]):\n",
    "    \"\"\"\n",
    "    根据按长度降序的BPE分词列表，将所给单词进行BPE分词，最后打印结果。\n",
    "    \n",
    "    思想是，对于一个待BPE分词的单词，按照长度顺序从列表中寻找BPE分词进行子串匹配，  \n",
    "    若成功匹配，则对该子串左右的剩余部分递归地进行下一轮匹配，直到剩余部分长度为0，  \n",
    "    或者剩余部分无法匹配（该部分整体由`\"<unknown>\"`代替）\n",
    "    \n",
    "    例1：  \n",
    "    输入 `word = \"supermarket\"`, `bpe_tokens=[(\"su\", 20), (\"are\", 10), (\"per\", 30)]`  \n",
    "    最终打印 `\"su per <unknown>\"`\n",
    "\n",
    "    例2：  \n",
    "    输入 `word = \"shanghai\"`, `bpe_tokens=[(\"hai\", 1), (\"sh\", 1), (\"an\", 1), (\"</w>\", 1), (\"g\", 1)]`  \n",
    "    最终打印 `\"sh an g hai </w>\"`\n",
    "\n",
    "    Args:\n",
    "        word: str - 待分词的单词\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPE分词和对应频数组成的列表\n",
    "    \"\"\"\n",
    "    \n",
    "    def bpe_tokenize(sub_word: str) -> str:\n",
    "        # 若剩余部分为空，则返回空字符串\n",
    "        if not sub_word:\n",
    "            return \"\"\n",
    "        \n",
    "        # 遍历 bpe_tokens 找到匹配的 token\n",
    "        for token, _ in bpe_tokens:\n",
    "            if sub_word.startswith(token):\n",
    "                # 匹配成功，递归处理剩余部分\n",
    "                remaining_result = bpe_tokenize(sub_word[len(token):])\n",
    "                return f\"{token} {remaining_result}\".strip()\n",
    "        \n",
    "        # 若无法匹配任何 token，则返回 \"<unknown>\" 并终止对当前子串的进一步处理，不进行进一步切分\n",
    "        return \"<unknown>\"\n",
    "    \n",
    "    # 开始递归分词并输出结果\n",
    "    res = bpe_tokenize(word + \"</w>\")\n",
    "    print(res.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8b0c",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcbaa0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh an g hai </w>\n"
     ]
    }
   ],
   "source": [
    "word = \"shanghai\"\n",
    "bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)]\n",
    "print_bpe_tokenize(word, bpe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f252c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su per <unknown>\n"
     ]
    }
   ],
   "source": [
    "word = \"supermarket\"\n",
    "bpe_tokens=[\n",
    "        (\"su\", 20),\n",
    "        (\"are\", 10),\n",
    "        (\"per\", 30),\n",
    "    ]\n",
    "print_bpe_tokenize(word, bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "开始读取数据集并训练BPE分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "215b56d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training corpus.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))\n",
    "\n",
    "print(\"Loaded training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf24fe3",
   "metadata": {},
   "source": [
    "训练与测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bccd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('demolishing</w>', 1), ('economists</w>', 1), ('monitoring</w>', 1), ('soliciting</w>', 1), ('agination</w>', 2), ('americans</w>', 5), ('asurement</w>', 1), ('economist</w>', 1), ('ministers</w>', 4), ('prominent</w>', 1), ('punishing</w>', 1), ('stination</w>', 1), ('stonecold</w>', 1), ('africans</w>', 1), ('american</w>', 11), ('asteland</w>', 1), ('becoming</w>', 1), ('benefits</w>', 2), ('decision</w>', 3), ('declined</w>', 1), ('dination</w>', 1), ('discount</w>', 2), ('discover</w>', 1), ('donation</w>', 1), ('finished</w>', 2), ('homeland</w>', 1), ('honoring</w>', 2), ('lowering</w>', 1), ('maritime</w>', 1), ('michelin</w>', 1), ('minister</w>', 16), ('minitour</w>', 1), ('national</w>', 25), ('parisons</w>', 1), ('promised</w>', 1), ('risoners</w>', 1), ('rouching</w>', 1), ('scheming</w>', 1), ('security</w>', 6), ('sination</w>', 1), ('sorority</w>', 1), ('timeline</w>', 1), ('tirement</w>', 1), ('tisement</w>', 1), ('tremists</w>', 1), ('verished</w>', 1), ('visiting</w>', 1), ('ability</w>', 5), ('acement</w>', 1), ('achable</w>', 1), ('adition</w>', 1), ('african</w>', 1), ('against</w>', 23), ('agement</w>', 9), ('alition</w>', 2), ('another</w>', 13), ('areared</w>', 1), ('asefire</w>', 1), ('ashamed</w>', 1), ('asoline</w>', 1), ('astrous</w>', 1), ('atement</w>', 10), ('aturely</w>', 1), ('aturing</w>', 1), ('aveling</w>', 1), ('becomes</w>', 2), ('benefit</w>', 3), ('chasing</w>', 2), ('decline</w>', 2), ('deficit</w>', 2), ('delines</w>', 1), ('dinitis</w>', 1), ('element</w>', 1), ('fearing</w>', 1), ('fishing</w>', 3), ('generes</w>', 1), ('hadists</w>', 1), ('hearing</w>', 2), ('homered</w>', 1), ('honored</w>', 2), ('irement</w>', 1), ('isition</w>', 2), ('limited</w>', 2), ('listing</w>', 1), ('marines</w>', 1), ('million</w>', 26), ('nations</w>', 1), ('nowhere</w>', 1), ('parison</w>', 1), ('pearing</w>', 1), ('pelican</w>', 1), ('plished</w>', 1), ('prefers</w>', 1), ('profile</w>', 1), ('promise</w>', 3), ('recover</w>', 1), ('richard</w>', 2), ('ronicle</w>', 1), ('scoring</w>', 2), ('secured</w>', 3), ('shaking</w>', 1), ('sharing</w>', 1), ('sistent</w>', 2), ('sisting</w>', 1), ('speople</w>', 1), ('stomers</w>', 5), ('thening</w>', 1), ('thering</w>', 1), ('touched</w>', 1), ('tourist</w>', 1), ('tremely</w>', 2), ('uniting</w>', 1), ('verines</w>', 1), ('visited</w>', 1), ('wearing</w>', 2), ('without</w>', 4), ('yearold</w>', 11), ('achers</w>', 1), ('aching</w>', 2), ('adited</w>', 1), ('ailers</w>', 2), ('ailing</w>', 3), ('aining</w>', 13), ('airing</w>', 1), ('aisers</w>', 1), ('aising</w>', 2), ('aiting</w>', 4), ('akened</w>', 1), ('alised</w>', 1), ('alises</w>', 1), ('alists</w>', 3), ('aminer</w>', 1), ('amount</w>', 5), ('anised</w>', 3), ('apolis</w>', 2), ('arecom</w>', 1), ('ascent</w>', 1), ('asered</w>', 1), ('ashing</w>', 2), ('asites</w>', 1), ('asoned</w>', 1), ('asting</w>', 3), ('asures</w>', 4), ('atened</w>', 2), ('athing</w>', 1), ('ations</w>', 33), ('atitis</w>', 1), ('atured</w>', 2), ('atures</w>', 3), ('aucous</w>', 1), ('aurant</w>', 2), ('become</w>', 5), ('before</w>', 12), ('bicles</w>', 1), ('bilisi</w>', 1), ('bility</w>', 2), ('bining</w>', 1), ('boring</w>', 1), ('cecily</w>', 1), ('cement</w>', 5), ('chased</w>', 1), ('citing</w>', 1), ('colour</w>', 1), ('coming</w>', 3), ('decent</w>', 2), ('deline</w>', 1), ('demand</w>', 3), ('dening</w>', 1), ('dering</w>', 1), ('dicine</w>', 2), ('dining</w>', 1), ('dishes</w>', 1), ('dition</w>', 7), ('ducers</w>', 1), ('ducing</w>', 2), ('ferent</w>', 5), ('fering</w>', 2), ('ficant</w>', 3), ('ficers</w>', 5), ('filing</w>', 1), ('finity</w>', 1), ('firing</w>', 1), ('gement</w>', 1), ('gering</w>', 1), ('gerous</w>', 1), ('gilant</w>', 2), ('habits</w>', 1), ('hailed</w>', 1), ('haring</w>', 1), ('hatred</w>', 1), ('havent</w>', 4), ('having</w>', 6), ('helich</w>', 1), ('hicles</w>', 3), ('hiring</w>', 1), ('homers</w>', 1), ('horans</w>', 1), ('jority</w>', 1), ('lecito</w>', 1), ('lement</w>', 4), ('licans</w>', 2), ('licity</w>', 3), ('limits</w>', 1), ('lining</w>', 1), ('lished</w>', 2), ('lishes</w>', 2), ('listed</w>', 2), ('lorist</w>', 1), ('marine</w>', 1), ('merely</w>', 1), ('merous</w>', 4), ('michel</w>', 1), ('mining</w>', 2), ('morist</w>', 1), ('mother</w>', 3), ('munist</w>', 2), ('munity</w>', 2), ('nation</w>', 2), ('nicole</w>', 2), ('nished</w>', 2), ('niture</w>', 1), ('parent</w>', 2), ('peared</w>', 6), ('pening</w>', 2), ('people</w>', 44), ('planes</w>', 3), ('police</w>', 15), ('ponent</w>', 1), ('profit</w>', 2), ('recent</w>', 5), ('refine</w>', 1), ('ricane</w>', 1), ('rimary</w>', 1), ('rising</w>', 4), ('risons</w>', 1), ('riting</w>', 1), ('ritish</w>', 5), ('rorist</w>', 1), ('ruling</w>', 4), ('scenes</w>', 1), ('scored</w>', 4), ('scores</w>', 2), ('secure</w>', 1), ('seling</w>', 2), ('sement</w>', 1), ('shared</w>', 2), ('shares</w>', 9), ('sharif</w>', 1), ('shires</w>', 1), ('should</w>', 15), ('showed</w>', 8), ('sisted</w>', 1), ('sister</w>', 2), ('sition</w>', 8), ('splant</w>', 1), ('sthits</w>', 1), ('stines</w>', 1), ('stomer</w>', 1), ('stones</w>', 1), ('stores</w>', 3), ('strand</w>', 1), ('string</w>', 1), ('sucher</w>', 1), ('surers</w>', 1), ('suring</w>', 2), ('tecuci</w>', 1), ('tering</w>', 2), ('theres</w>', 2), ('thored</w>', 1), ('timely</w>', 1), ('timing</w>', 2), ('timore</w>', 2), ('tition</w>', 2), ('towels</w>', 1), ('turers</w>', 3), ('turing</w>', 3), ('tworun</w>', 1), ('uclear</w>', 4), ('united</w>', 16), ('veland</w>', 1), ('vement</w>', 7), ('verano</w>', 1), ('verely</w>', 1), ('vering</w>', 3), ('vising</w>', 1), ('vision</w>', 10), ('visits</w>', 1), ('werent</w>', 1), ('within</w>', 5), ('younis</w>', 1), ('abour</w>', 3), ('about</w>', 41), ('ached</w>', 3), ('acher</w>', 1), ('aches</w>', 3), ('acing</w>', 3), ('acity</w>', 2), ('acles</w>', 1), ('aders</w>', 7), ('ading</w>', 12), ('afers</w>', 1), ('after</w>', 44), ('again</w>', 7), ('agels</w>', 1), ('agers</w>', 5), ('agine</w>', 1), ('aging</w>', 2), ('ailed</w>', 7), ('ailer</w>', 3), ('aimed</w>', 2), ('ained</w>', 17), ('aires</w>', 1), ('aised</w>', 6), ('aiser</w>', 1), ('aises</w>', 1), ('akers</w>', 6), ('aking</w>', 17), ('alant</w>', 1), ('alent</w>', 3), ('alice</w>', 1), ('alili</w>', 1), ('aling</w>', 3), ('alino</w>', 1), ('alist</w>', 4), ('ality</w>', 7), ('amels</w>', 1), ('ament</w>', 6), ('amers</w>', 1), ('amily</w>', 7), ('amine</w>', 3), ('aming</w>', 3), ('amous</w>', 4), ('aning</w>', 2), ('anish</w>', 2), ('apers</w>', 4), ('apled</w>', 1), ('apons</w>', 1), ('arily</w>', 3), ('aring</w>', 2), ('ascus</w>', 1), ('ashir</w>', 1), ('asily</w>', 4), ('asing</w>', 3), ('asite</w>', 2), ('asons</w>', 3), ('aster</w>', 3), ('astly</w>', 1), ('aston</w>', 1), ('astro</w>', 1), ('asure</w>', 2), ('ately</w>', 4), ('athan</w>', 2), ('ather</w>', 9), ('ating</w>', 24), ('ation</w>', 99), ('ature</w>', 5), ('avier</w>', 1), ('avily</w>', 1), ('aving</w>', 5), ('avish</w>', 1), ('ayers</w>', 6), ('bears</w>', 1), ('below</w>', 2), ('berus</w>', 2), ('bined</w>', 2), ('bited</w>', 2), ('bowen</w>', 1), ('chain</w>', 1), ('chair</w>', 1), ('chard</w>', 1), ('chase</w>', 2), ('chemo</w>', 1), ('chers</w>', 1), ('child</w>', 8), ('chile</w>', 1), ('chill</w>', 2), ('ching</w>', 8), ('cited</w>', 3), ('clear</w>', 8), ('cling</w>', 1), ('comed</w>', 1), ('comes</w>', 4), ('could</w>', 19), ('count</w>', 4), ('cover</w>', 2), ('cured</w>', 1), ('decor</w>', 1), ('dened</w>', 2), ('dered</w>', 8), ('dimes</w>', 1), ('dines</w>', 1), ('dited</w>', 4), ('ditor</w>', 3), ('dolan</w>', 1), ('domed</w>', 1), ('doned</w>', 1), ('donor</w>', 1), ('duced</w>', 4), ('ducer</w>', 3), ('duces</w>', 1), ('eling</w>', 3), ('elite</w>', 2), ('ement</w>', 4), ('eming</w>', 1), ('ening</w>', 1), ('ering</w>', 5), ('erome</w>', 1), ('fears</w>', 1), ('fered</w>', 4), ('ficer</w>', 4), ('fices</w>', 2), ('filed</w>', 2), ('fined</w>', 1), ('fired</w>', 2), ('first</w>', 26), ('geles</w>', 2), ('genes</w>', 1), ('gered</w>', 3), ('ghold</w>', 1), ('ghout</w>', 6), ('gines</w>', 1), ('gists</w>', 2), ('grand</w>', 2), ('grant</w>', 3), ('grity</w>', 1), ('group</w>', 11), ('gures</w>', 2), ('halis</w>', 1), ('hamed</w>', 2), ('hamit</w>', 1), ('hasis</w>', 1), ('heard</w>', 3), ('helen</w>', 1), ('henin</w>', 1), ('hicle</w>', 2), ('hired</w>', 1), ('hiron</w>', 1), ('homer</w>', 1), ('homes</w>', 5), ('hones</w>', 1), ('icher</w>', 1), ('imity</w>', 1), ('ining</w>', 2), ('inter</w>', 3), ('irans</w>', 1), ('iring</w>', 2), ('ither</w>', 3), ('jones</w>', 1), ('jured</w>', 4), ('lican</w>', 1), ('limit</w>', 1), ('lined</w>', 1), ('lines</w>', 2), ('lowed</w>', 4), ('lower</w>', 4), ('milan</w>', 1), ('miles</w>', 5), ('mirel</w>', 1), ('mored</w>', 1), ('mount</w>', 1), ('niman</w>', 1), ('nited</w>', 1), ('nomis</w>', 1), ('nored</w>', 1), ('olent</w>', 3), ('oning</w>', 4), ('orite</w>', 2), ('ority</w>', 1), ('other</w>', 37), ('pared</w>', 9), ('pears</w>', 1), ('pened</w>', 3), ('pered</w>', 3), ('peres</w>', 1), ('plane</w>', 4), ('plans</w>', 6), ('plant</w>', 3), ('plene</w>', 1), ('pline</w>', 1), ('poles</w>', 1), ('power</w>', 7), ('preme</w>', 2), ('reary</w>', 1), ('rices</w>', 5), ('rimes</w>', 1), ('riser</w>', 1), ('rises</w>', 1), ('risis</w>', 3), ('rison</w>', 1), ('riter</w>', 1), ('rites</w>', 1), ('rored</w>', 1), ('rover</w>', 1), ('ruled</w>', 3), ('ruler</w>', 1), ('rules</w>', 3), ('scene</w>', 3), ('scent</w>', 1), ('scher</w>', 1), ('score</w>', 2), ('shall</w>', 1), ('shame</w>', 1), ('shape</w>', 1), ('share</w>', 7), ('shave</w>', 1), ('shene</w>', 1), ('shing</w>', 5), ('shire</w>', 1), ('shore</w>', 4), ('sists</w>', 1), ('sites</w>', 2), ('sonal</w>', 2), ('sored</w>', 1), ('spent</w>', 3), ('stars</w>', 2), ('sters</w>', 4), ('stice</w>', 3), ('still</w>', 8), ('sting</w>', 5), ('stint</w>', 1), ('stion</w>', 4), ('stone</w>', 3), ('store</w>', 5), ('strom</w>', 1), ('stunt</w>', 1), ('sunil</w>', 1), ('sured</w>', 1), ('surer</w>', 1), ('sures</w>', 1), ('tened</w>', 2), ('tenor</w>', 1), ('teran</w>', 1), ('tered</w>', 2), ('thats</w>', 7), ('their</w>', 56), ('theme</w>', 1), ('there</w>', 33), ('thers</w>', 9), ('thing</w>', 14), ('ticed</w>', 1), ('tices</w>', 1), ('ticle</w>', 2), ('tiles</w>', 1), ('times</w>', 7), ('tired</w>', 1), ('tires</w>', 2), ('tists</w>', 2), ('tonin</w>', 1), ('touch</w>', 2), ('towel</w>', 1), ('treme</w>', 1), ('trols</w>', 1), ('trout</w>', 1), ('truro</w>', 1), ('tured</w>', 2), ('tures</w>', 5), ('ulent</w>', 1), ('units</w>', 3), ('uring</w>', 14), ('venir</w>', 1), ('vered</w>', 1), ('vices</w>', 14), ('virus</w>', 1), ('vised</w>', 1), ('visit</w>', 4), ('vists</w>', 2), ('wered</w>', 1), ('where</w>', 18), ('which</w>', 49), ('while</w>', 24), ('white</w>', 8), ('whole</w>', 1), ('would</w>', 49), ('years</w>', 29), ('youre</w>', 5), ('abel</w>', 2), ('able</w>', 38), ('abon</w>', 1), ('abor</w>', 2), ('abul</w>', 1), ('aced</w>', 6), ('aces</w>', 4), ('ache</w>', 1), ('achi</w>', 1), ('acho</w>', 1), ('acle</w>', 3), ('acre</w>', 1), ('aded</w>', 4), ('aden</w>', 1), ('ader</w>', 8), ('ades</w>', 1), ('ador</w>', 2), ('afer</w>', 1), ('aged</w>', 5), ('agen</w>', 1), ('ager</w>', 4), ('ages</w>', 9), ('agon</w>', 1), ('ails</w>', 4), ('aily</w>', 1), ('aint</w>', 1), ('aire</w>', 1), ('aise</w>', 5), ('aisy</w>', 1), ('aith</w>', 1), ('aits</w>', 1), ('aken</w>', 7), ('aker</w>', 3), ('akes</w>', 11), ('aler</w>', 1), ('ales</w>', 8), ('alex</w>', 3), ('alif</w>', 1), ('alin</w>', 2), ('ally</w>', 42), ('also</w>', 44), ('amed</w>', 8), ('ames</w>', 11), ('amex</w>', 1), ('amin</w>', 3), ('amis</w>', 1), ('anel</w>', 3), ('aner</w>', 1), ('anor</w>', 1), ('aper</w>', 6), ('aren</w>', 1), ('arim</w>', 1), ('aron</w>', 2), ('arun</w>', 1), ('ased</w>', 25), ('ases</w>', 16), ('asif</w>', 1), ('asir</w>', 1), ('asis</w>', 2), ('ason</w>', 11), ('asts</w>', 1), ('ated</w>', 50), ('aten</w>', 1), ('ater</w>', 25), ('ates</w>', 43), ('atin</w>', 3), ('ator</w>', 11), ('auls</w>', 1), ('avel</w>', 2), ('aven</w>', 1), ('aves</w>', 1), ('away</w>', 11), ('awed</w>', 1), ('ayed</w>', 5), ('ayer</w>', 2), ('bear</w>', 1), ('been</w>', 37), ('belichic', 1), ('benefici', 1), ('bers</w>', 10), ('bile</w>', 3), ('bill</w>', 4), ('bine</w>', 1), ('bing</w>', 4), ('bone</w>', 1), ('cant</w>', 1), ('card</w>', 1), ('care</w>', 10), ('cars</w>', 2), ('celo</w>', 1), ('cely</w>', 1), ('cent</w>', 28), ('cers</w>', 1), ('chad</w>', 1), ('char</w>', 1), ('chat</w>', 1), ('ched</w>', 5), ('chen</w>', 2), ('cher</w>', 1), ('ches</w>', 5), ('cils</w>', 1), ('cing</w>', 9), ('city</w>', 11), ('clinicli', 1), ('cole</w>', 1), ('come</w>', 19), ('dels</w>', 1), ('dely</w>', 1), ('dent</w>', 36), ('ders</w>', 16), ('ding</w>', 70), ('dire</w>', 1), ('dise</w>', 1), ('dish</w>', 2), ('dits</w>', 1), ('done</w>', 4), ('dont</w>', 14), ('duce</w>', 6), ('ears</w>', 1), ('eces</w>', 1), ('economic', 7), ('eled</w>', 1), ('eler</w>', 1), ('emed</w>', 6), ('ener</w>', 1), ('ered</w>', 2), ('fear</w>', 5), ('fers</w>', 3), ('fice</w>', 10), ('file</w>', 2), ('fine</w>', 1), ('fing</w>', 1), ('fire</w>', 5), ('fith</w>', 1), ('fiti</w>', 1), ('from</w>', 99), ('gear</w>', 2), ('gely</w>', 5), ('gent</w>', 1), ('gers</w>', 8), ('gham</w>', 1), ('ghan</w>', 3), ('gher</w>', 6), ('gime</w>', 2), ('ging</w>', 4), ('gist</w>', 1), ('gold</w>', 2), ('gone</w>', 3), ('gore</w>', 1), ('grim</w>', 1), ('grow</w>', 4), ('gure</w>', 2), ('habi</w>', 1), ('hail</w>', 1), ('hair</w>', 1), ('hall</w>', 1), ('halo</w>', 1), ('hand</w>', 1), ('hany</w>', 1), ('haps</w>', 3), ('hard</w>', 11), ('hats</w>', 1), ('have</w>', 92), ('held</w>', 8), ('here</w>', 12), ('hers</w>', 1), ('hill</w>', 1), ('hire</w>', 2), ('hist</w>', 1), ('historic', 2), ('hits</w>', 2), ('hold</w>', 4), ('hole</w>', 1), ('home</w>', 15), ('hone</w>', 6), ('hour</w>', 5), ('hunt</w>', 3), ('iced</w>', 1), ('ices</w>', 1), ('icon</w>', 1), ('iled</w>', 1), ('iler</w>', 2), ('ilis</w>', 1), ('ined</w>', 3), ('ines</w>', 2), ('into</w>', 36), ('iran</w>', 4), ('ised</w>', 1), ('ited</w>', 1), ('item</w>', 1), ('ites</w>', 1), ('july</w>', 4), ('june</w>', 5), ('kely</w>', 8), ('kent</w>', 1), ('kers</w>', 8), ('king</w>', 33), ('land</w>', 15), ('lers</w>', 5), ('line</w>', 17), ('ling</w>', 32), ('lint</w>', 1), ('lion</w>', 7), ('lish</w>', 2), ('list</w>', 4), ('lout</w>', 2), ('mand</w>', 3), ('mans</w>', 2), ('many</w>', 14), ('mars</w>', 2), ('mary</w>', 2), ('mechanis', 1), ('memo</w>', 1), ('ment</w>', 69), ('mers</w>', 5), ('mich</w>', 1), ('mild</w>', 1), ('mile</w>', 1), ('mine</w>', 1), ('ming</w>', 5), ('mino</w>', 1), ('miss</w>', 3), ('mits</w>', 2), ('mons</w>', 2), ('mont</w>', 1), ('more</w>', 50), ('most</w>', 29), ('mous</w>', 1), ('much</w>', 8), ('near</w>', 5), ('nels</w>', 1), ('ners</w>', 6), ('nine</w>', 7), ('ning</w>', 20), ('none</w>', 1), ('oled</w>', 1), ('oman</w>', 10), ('omed</w>', 1), ('omen</w>', 10), ('omer</w>', 1), ('onal</w>', 19), ('oned</w>', 8), ('onen</w>', 1), ('oner</w>', 2), ('ones</w>', 2), ('only</w>', 23), ('ouls</w>', 1), ('over</w>', 30), ('pear</w>', 2), ('pers</w>', 5), ('plan</w>', 9), ('pler</w>', 1), ('ples</w>', 4), ('plex</w>', 3), ('pole</w>', 1), ('politici', 3), ('poly</w>', 1), ('port</w>', 30), ('prelimin', 1), ('prominen', 1), ('pure</w>', 2), ('rand</w>', 4), ('rear</w>', 2), ('rent</w>', 12), ('rice</w>', 10), ('rich</w>', 3), ('rill</w>', 1), ('rime</w>', 8), ('rine</w>', 1), ('ring</w>', 11), ('rini</w>', 1), ('rise</w>', 5), ('rist</w>', 2), ('rite</w>', 1), ('role</w>', 4), ('rome</w>', 1), ('ront</w>', 5), ('rowe</w>', 1), ('rule</w>', 2), ('runo</w>', 1), ('said</w>', 152), ('scle</w>', 1), ('scow</w>', 1), ('sday</w>', 28), ('secrecor', 1), ('securiti', 2), ('sels</w>', 2), ('sent</w>', 6), ('sers</w>', 2), ('shed</w>', 4), ('shes</w>', 2), ('show</w>', 7), ('sile</w>', 2), ('sing</w>', 21), ('sion</w>', 23), ('sist</w>', 2), ('site</w>', 10), ('sity</w>', 9), ('sold</w>', 3), ('some</w>', 27), ('sons</w>', 5), ('sore</w>', 1), ('star</w>', 4), ('sted</w>', 8), ('stem</w>', 9), ('ster</w>', 5), ('stin</w>', 1), ('stir</w>', 1), ('stly</w>', 1), ('ston</w>', 9), ('stor</w>', 2), ('such</w>', 11), ('sure</w>', 13), ('tear</w>', 1), ('tech</w>', 2), ('tele</w>', 1), ('tels</w>', 2), ('tely</w>', 5), ('tent</w>', 2), ('ters</w>', 16), ('than</w>', 31), ('that</w>', 191), ('them</w>', 28), ('then</w>', 16), ('ther</w>', 11), ('they</w>', 72), ('this</w>', 69), ('thor</w>', 1), ('thur</w>', 1), ('tice</w>', 4), ('time</w>', 30), ('tine</w>', 1), ('ting</w>', 75), ('tion</w>', 64), ('tire</w>', 2), ('tise</w>', 3), ('tish</w>', 2), ('tist</w>', 6), ('tite</w>', 1), ('tity</w>', 2), ('told</w>', 12), ('toni</w>', 1), ('tons</w>', 2), ('tour</w>', 1), ('tous</w>', 1), ('trol</w>', 2), ('tune</w>', 2), ('ture</w>', 16), ('uled</w>', 2), ('uran</w>', 3), ('ured</w>', 1), ('ures</w>', 2), ('vels</w>', 2), ('vely</w>', 7), ('vent</w>', 7), ('vere</w>', 1), ('vers</w>', 12), ('very</w>', 28), ('vice</w>', 19), ('vili</w>', 1), ('vine</w>', 2), ('ving</w>', 28), ('vino</w>', 1), ('vist</w>', 2), ('wear</w>', 2), ('went</w>', 6), ('were</w>', 51), ('wers</w>', 3), ('what</w>', 16), ('when</w>', 47), ('whom</w>', 1), ('will</w>', 70), ('with</w>', 155), ('wore</w>', 1), ('year</w>', 32), ('yers</w>', 2), ('your</w>', 12), ('200</w>', 1), ('abiliti', 6), ('aby</w>', 2), ('ace</w>', 35), ('ach</w>', 37), ('achicol', 2), ('aciliti', 1), ('ack</w>', 34), ('aco</w>', 1), ('ade</w>', 20), ('adi</w>', 3), ('ado</w>', 1), ('ads</w>', 13), ('ady</w>', 9), ('afe</w>', 2), ('african', 1), ('age</w>', 45), ('ago</w>', 10), ('agricul', 1), ('ags</w>', 1), ('aid</w>', 5), ('ail</w>', 9), ('aim</w>', 3), ('ain</w>', 32), ('air</w>', 18), ('ait</w>', 1), ('ake</w>', 48), ('ald</w>', 1), ('ale</w>', 7), ('ali</w>', 3), ('all</w>', 64), ('alo</w>', 2), ('als</w>', 32), ('aly</w>', 2), ('amanati', 1), ('ame</w>', 43), ('ami</w>', 4), ('and</w>', 516), ('ane</w>', 1), ('ani</w>', 5), ('ano</w>', 3), ('ans</w>', 19), ('ant</w>', 21), ('any</w>', 24), ('ape</w>', 2), ('aps</w>', 2), ('ard</w>', 32), ('are</w>', 86), ('ari</w>', 1), ('aro</w>', 1), ('ars</w>', 8), ('ary</w>', 35), ('ase</w>', 37), ('ash</w>', 10), ('asi</w>', 1), ('ass</w>', 11), ('ast</w>', 44), ('asy</w>', 4), ('ate</w>', 91), ('ath</w>', 11), ('atholic', 1), ('ato</w>', 2), ('ats</w>', 7), ('aul</w>', 3), ('aus</w>', 1), ('ave</w>', 12), ('ays</w>', 37), ('bed</w>', 7), ('bel</w>', 1), ('ber</w>', 44), ('bes</w>', 1), ('bim</w>', 1), ('bol</w>', 1), ('bon</w>', 2), ('bor</w>', 1), ('bus</w>', 1), ('but</w>', 70), ('can</w>', 34), ('car</w>', 6), ('carolin', 4), ('ced</w>', 15), ('cer</w>', 6), ('ceremon', 3), ('ces</w>', 16), ('che</w>', 1), ('cil</w>', 5), ('cle</w>', 2), ('clearan', 1), ('col</w>', 1), ('com</w>', 13), ('cor</w>', 1), ('cup</w>', 4), ('day</w>', 61), ('ded</w>', 59), ('del</w>', 3), ('den</w>', 6), ('der</w>', 37), ('des</w>', 11), ('dex</w>', 4), ('din</w>', 1), ('dis</w>', 2), ('distric', 8), ('dit</w>', 5), ('dol</w>', 1), ('dom</w>', 1), ('don</w>', 10), ('dor</w>', 1), ('ear</w>', 2), ('ece</w>', 3), ('ech</w>', 2), ('eld</w>', 5), ('ele</w>', 1), ('eli</w>', 3), ('els</w>', 3), ('ene</w>', 1), ('ent</w>', 6), ('ere</w>', 2), ('ero</w>', 1), ('ers</w>', 9), ('fed</w>', 5), ('fer</w>', 5), ('fit</w>', 3), ('for</w>', 208), ('foreclo', 2), ('ged</w>', 20), ('gel</w>', 1), ('generic', 1), ('ger</w>', 16), ('ges</w>', 13), ('get</w>', 26), ('ghanist', 4), ('ghi</w>', 1), ('ght</w>', 70), ('gin</w>', 4), ('gis</w>', 1), ('gon</w>', 4), ('gor</w>', 1), ('gul</w>', 3), ('gun</w>', 3), ('had</w>', 51), ('ham</w>', 2), ('han</w>', 2), ('har</w>', 1), ('has</w>', 72), ('hen</w>', 2), ('her</w>', 37), ('hes</w>', 10), ('hil</w>', 1), ('him</w>', 13), ('his</w>', 100), ('hit</w>', 4), ('hol</w>', 2), ('how</w>', 15), ('ice</w>', 7), ('ico</w>', 3), ('ild</w>', 4), ('ile</w>', 1), ('ill</w>', 3), ('ine</w>', 1), ('ing</w>', 98), ('int</w>', 10), ('ire</w>', 6), ('ise</w>', 3), ('ish</w>', 4), ('isi</w>', 1), ('iss</w>', 1), ('isy</w>', 1), ('ite</w>', 9), ('ith</w>', 2), ('its</w>', 66), ('ity</w>', 6), ('jon</w>', 1), ('jor</w>', 4), ('jorisch', 1), ('ked</w>', 27), ('ken</w>', 3), ('ker</w>', 6), ('lan</w>', 2), ('led</w>', 37), ('lel</w>', 1), ('lem</w>', 3), ('len</w>', 5), ('ler</w>', 10), ('les</w>', 9), ('lex</w>', 1), ('lin</w>', 2), ('lis</w>', 1), ('lon</w>', 1), ('low</w>', 13), ('man</w>', 44), ('mar</w>', 7), ('med</w>', 13), ('men</w>', 9), ('mer</w>', 19), ('mes</w>', 1), ('mex</w>', 1), ('ministr', 4), ('mit</w>', 2), ('mom</w>', 1), ('mon</w>', 4), ('ned</w>', 18), ('nel</w>', 4), ('ner</w>', 15), ('nes</w>', 2), ('new</w>', 70), ('nil</w>', 1), ('nis</w>', 1), ('nor</w>', 1), ('not</w>', 55), ('now</w>', 31), ('old</w>', 5), ('ole</w>', 1), ('ols</w>', 7), ('one</w>', 46), ('ons</w>', 48), ('ont</w>', 3), ('oul</w>', 1), ('our</w>', 41), ('ous</w>', 12), ('out</w>', 38), ('par</w>', 2), ('ped</w>', 16), ('pen</w>', 6), ('per</w>', 20), ('perimen', 2), ('pes</w>', 5), ('ple</w>', 11), ('pli</w>', 1), ('ply</w>', 8), ('pol</w>', 1), ('politic', 9), ('pronoun', 1), ('pur</w>', 1), ('pus</w>', 2), ('ran</w>', 10), ('red</w>', 9), ('referen', 1), ('rel</w>', 4), ('ren</w>', 14), ('renomin', 1), ('res</w>', 2), ('ril</w>', 3), ('ris</w>', 7), ('rit</w>', 1), ('ron</w>', 2), ('ror</w>', 3), ('row</w>', 7), ('run</w>', 8), ('sch</w>', 2), ('sed</w>', 49), ('sef</w>', 1), ('sel</w>', 2), ('sen</w>', 7), ('ser</w>', 4), ('ses</w>', 27), ('sex</w>', 2), ('shamele', 1), ('she</w>', 35), ('shi</w>', 1), ('sin</w>', 1), ('sis</w>', 5), ('sit</w>', 4), ('son</w>', 28), ('sor</w>', 2), ('stiliti', 1), ('sto</w>', 1), ('sts</w>', 18), ('sty</w>', 1), ('sun</w>', 1), ('sus</w>', 2), ('ted</w>', 101), ('tel</w>', 3), ('ten</w>', 15), ('ter</w>', 45), ('tes</w>', 12), ('tex</w>', 1), ('the</w>', 1218), ('thoriti', 6), ('til</w>', 5), ('tim</w>', 2), ('tin</w>', 5), ('tis</w>', 2), ('tly</w>', 26), ('toleran', 1), ('tom</w>', 1), ('ton</w>', 18), ('tor</w>', 18), ('tre</w>', 2), ('tro</w>', 1), ('tup</w>', 1), ('tus</w>', 1), ('two</w>', 31), ('ure</w>', 3), ('uro</w>', 2), ('ved</w>', 34), ('vel</w>', 7), ('ven</w>', 27), ('ver</w>', 36), ('veronic', 1), ('ves</w>', 25), ('vicemem', 1), ('vie</w>', 4), ('vil</w>', 4), ('vin</w>', 3), ('vis</w>', 1), ('was</w>', 121), ('way</w>', 24), ('wed</w>', 4), ('wen</w>', 1), ('wer</w>', 3), ('who</w>', 57), ('yed</w>', 4), ('yer</w>', 2), ('you</w>', 39), ('00</w>', 26), ('ab</w>', 4), ('acilit', 2), ('ad</w>', 59), ('ag</w>', 3), ('ai</w>', 3), ('ak</w>', 5), ('al</w>', 251), ('aloric', 1), ('am</w>', 38), ('americ', 7), ('an</w>', 170), ('ap</w>', 13), ('ar</w>', 38), ('as</w>', 173), ('astron', 1), ('astruc', 3), ('at</w>', 134), ('atemen', 3), ('atholo', 2), ('atrici', 1), ('aw</w>', 18), ('ay</w>', 51), ('ayerow', 1), ('be</w>', 100), ('bi</w>', 3), ('bishof', 1), ('bo</w>', 1), ('by</w>', 105), ('ce</w>', 123), ('ch</w>', 33), ('charit', 1), ('chitec', 2), ('ci</w>', 1), ('clinic', 2), ('co</w>', 3), ('coloni', 1), ('de</w>', 52), ('decisi', 2), ('defici', 1), ('di</w>', 2), ('disclo', 1), ('discon', 1), ('discre', 2), ('disper', 1), ('disten', 1), ('distin', 1), ('distri', 4), ('do</w>', 16), ('ds</w>', 108), ('dy</w>', 25), ('econom', 6), ('ed</w>', 63), ('ef</w>', 12), ('el</w>', 21), ('elemen', 3), ('elimin', 2), ('em</w>', 2), ('en</w>', 31), ('er</w>', 31), ('es</w>', 155), ('fe</w>', 16), ('fisher', 1), ('ge</w>', 61), ('gh</w>', 47), ('gistic', 1), ('gn</w>', 28), ('go</w>', 18), ('gs</w>', 32), ('gy</w>', 26), ('ha</w>', 1), ('he</w>', 117), ('helico', 3), ('hemisp', 1), ('histor', 2), ('ho</w>', 3), ('homici', 1), ('if</w>', 30), ('il</w>', 11), ('im</w>', 11), ('in</w>', 431), ('intere', 12), ('interi', 2), ('ir</w>', 1), ('iremen', 2), ('is</w>', 165), ('it</w>', 103), ('jo</w>', 1), ('ke</w>', 36), ('ks</w>', 44), ('ld</w>', 13), ('le</w>', 70), ('listic', 2), ('lo</w>', 2), ('ls</w>', 31), ('ly</w>', 71), ('me</w>', 22), ('militi', 2), ('minino', 1), ('miscom', 1), ('miscon', 1), ('mr</w>', 28), ('muniti', 2), ('ne</w>', 7), ('ni</w>', 3), ('nichol', 1), ('nicole', 1), ('no</w>', 26), ('nt</w>', 24), ('of</w>', 536), ('ol</w>', 12), ('om</w>', 11), ('on</w>', 218), ('or</w>', 66), ('ow</w>', 1), ('pe</w>', 10), ('pearan', 1), ('perime', 1), ('pl</w>', 1), ('po</w>', 2), ('polici', 1), ('prefer', 2), ('ps</w>', 31), ('punish', 1), ('re</w>', 4), ('ri</w>', 3), ('richar', 1), ('riteri', 1), ('ro</w>', 1), ('scener', 1), ('se</w>', 137), ('sh</w>', 17), ('si</w>', 3), ('simili', 1), ('sisten', 1), ('so</w>', 23), ('sparen', 1), ('sparin', 1), ('ss</w>', 109), ('st</w>', 109), ('stican', 1), ('stimon', 2), ('sy</w>', 2), ('te</w>', 31), ('th</w>', 91), ('thorou', 1), ('ti</w>', 3), ('to</w>', 534), ('touris', 1), ('ts</w>', 202), ('ty</w>', 45), ('ul</w>', 11), ('un</w>', 7), ('up</w>', 26), ('ur</w>', 1), ('us</w>', 53), ('ve</w>', 108), ('vi</w>', 1), ('we</w>', 56), ('ye</w>', 3), ('0</w>', 108), ('1</w>', 48), ('2</w>', 60), ('3</w>', 58), ('4</w>', 51), ('5</w>', 55), ('6</w>', 40), ('7</w>', 48), ('a</w>', 728), ('abili', 1), ('aditi', 5), ('afric', 2), ('aisic', 1), ('alori', 4), ('amero', 1), ('amili', 4), ('amoun', 2), ('aneri', 1), ('apolo', 1), ('arene', 3), ('ariti', 1), ('aroun', 13), ('ascen', 1), ('ashin', 10), ('aspec', 1), ('astic', 3), ('astro', 1), ('athol', 1), ('atolo', 1), ('atric', 2), ('auren', 1), ('b</w>', 30), ('belon', 1), ('biche', 1), ('bilin', 1), ('bisec', 1), ('bisho', 1), ('bolic', 1), ('bulan', 1), ('caref', 1), ('celer', 2), ('cemen', 1), ('chair', 5), ('chene', 1), ('chine', 4), ('chiri', 1), ('cholo', 1), ('chore', 2), ('clear', 2), ('cleli', 1), ('color', 1), ('comen', 1), ('d</w>', 215), ('decem', 2), ('decon', 1), ('defen', 13), ('defer', 1), ('defor', 1), ('delin', 1), ('deman', 4), ('demen', 1), ('demon', 3), ('direc', 13), ('disci', 2), ('disco', 2), ('discu', 2), ('displ', 2), ('dispu', 2), ('ditor', 1), ('dolef', 1), ('e</w>', 112), ('enemi', 1), ('f</w>', 45), ('felon', 1), ('femou', 1), ('feren', 13), ('ferin', 1), ('ficul', 1), ('filin', 1), ('g</w>', 69), ('gemen', 1), ('gener', 11), ('gerow', 1), ('gomer', 1), ('groun', 9), ('habit', 1), ('hagic', 1), ('hemin', 1), ('hemis', 2), ('hemor', 1), ('herit', 1), ('hilan', 1), ('honor', 1), ('i</w>', 82), ('inclu', 23), ('inter', 30), ('inven', 3), ('inver', 1), ('irani', 1), ('irele', 2), ('juror', 2), ('k</w>', 184), ('l</w>', 54), ('lecor', 1), ('licen', 4), ('licic', 1), ('limit', 1), ('liter', 3), ('lowel', 1), ('m</w>', 60), ('memor', 3), ('micha', 4), ('michi', 3), ('milit', 8), ('momen', 2), ('munic', 5), ('n</w>', 96), ('nomin', 1), ('noran', 1), ('o</w>', 33), ('oriti', 1), ('p</w>', 83), ('paren', 6), ('polic', 4), ('polis', 1), ('power', 2), ('prece', 1), ('preci', 2), ('premi', 7), ('presi', 24), ('profe', 6), ('promo', 4), ('pulit', 1), ('r</w>', 12), ('recen', 4), ('recom', 3), ('recon', 3), ('recor', 13), ('refer', 2), ('refor', 3), ('remin', 3), ('remis', 1), ('rimin', 2), ('ritic', 2), ('ritor', 1), ('roris', 2), ('s</w>', 279), ('schaf', 1), ('schem', 1), ('schin', 1), ('schol', 1), ('schul', 2), ('score', 1), ('secon', 11), ('secre', 3), ('selec', 2), ('semin', 1), ('share', 4), ('shoul', 1), ('simil', 6), ('solan', 1), ('speci', 25), ('stear', 1), ('stero', 1), ('stini', 3), ('stitu', 7), ('stran', 3), ('stren', 1), ('stric', 2), ('stron', 3), ('struc', 11), ('suran', 5), ('t</w>', 341), ('terin', 1), ('thear', 1), ('there', 1), ('tholo', 1), ('thori', 2), ('ticul', 2), ('toler', 1), ('toric', 2), ('torol', 1), ('toron', 2), ('touch', 4), ('trole', 1), ('unori', 1), ('urani', 2), ('urolo', 1), ('vemen', 1), ('verin', 1), ('vinen', 1), ('viron', 3), ('visor', 2), ('w</w>', 37), ('whate', 4), ('whene', 2), ('where', 1), ('white', 1), ('whole', 1), ('x</w>', 30), ('y</w>', 242), ('</w>', 235), ('abel', 2), ('abor', 2), ('abun', 1), ('acan', 1), ('achi', 2), ('acho', 1), ('achu', 1), ('acis', 1), ('acou', 1), ('acul', 1), ('adel', 2), ('ader', 1), ('adic', 3), ('adil', 1), ('adon', 1), ('adou', 1), ('agen', 7), ('agit', 2), ('agre', 8), ('aile', 1), ('aine', 2), ('aist', 1), ('alan', 5), ('alen', 1), ('alex', 1), ('alin', 1), ('alis', 1), ('alit', 1), ('alon', 4), ('aman', 1), ('amen', 4), ('amer', 3), ('amic', 2), ('amil', 3), ('amin', 2), ('amon', 12), ('amor', 2), ('amou', 1), ('amul', 1), ('anec', 1), ('anic', 4), ('anim', 1), ('anin', 1), ('anis', 1), ('apar', 1), ('apel', 1), ('asci', 1), ('ashi', 3), ('asic', 3), ('asin', 1), ('ason', 2), ('aste', 1), ('asye', 1), ('aten', 1), ('ater', 3), ('athi', 1), ('atic', 7), ('atin', 1), ('atis', 3), ('ator', 10), ('atri', 2), ('atur', 8), ('aure', 1), ('avel', 1), ('aver', 3), ('avil', 1), ('avin', 1), ('ayer', 1), ('bech', 1), ('beli', 13), ('bere', 1), ('bitr', 1), ('boun', 2), ('bour', 1), ('bure', 1), ('buri', 1), ('cane', 1), ('care', 1), ('cele', 2), ('chal', 2), ('cham', 6), ('chan', 22), ('chap', 1), ('char', 13), ('chat', 1), ('chec', 5), ('chel', 2), ('chic', 5), ('chil', 13), ('chin', 9), ('chit', 1), ('chow', 1), ('chun', 1), ('chur', 5), ('cili', 2), ('cine', 1), ('cisi', 1), ('citi', 1), ('clel', 1), ('clem', 1), ('clic', 1), ('clim', 4), ('clin', 4), ('clow', 1), ('coul', 3), ('coun', 45), ('cour', 18), ('curi', 1), ('deci', 3), ('decl', 1), ('defe', 2), ('dele', 1), ('deli', 6), ('demo', 11), ('deni', 5), ('dere', 2), ('deri', 1), ('dici', 2), ('disc', 1), ('dise', 4), ('dist', 2), ('diti', 9), ('doli', 1), ('dome', 3), ('done', 1), ('echo', 1), ('efin', 1), ('elec', 8), ('elen', 1), ('emer', 2), ('emic', 1), ('emir', 2), ('emis', 2), ('emon', 1), ('ener', 4), ('enor', 1), ('enou', 6), ('erem', 1), ('eric', 1), ('feri', 1), ('fero', 1), ('fici', 16), ('fire', 1), ('fisc', 2), ('fore', 10), ('foru', 1), ('gene', 2), ('geri', 4), ('gham', 1), ('gine', 4), ('gole', 1), ('gran', 5), ('grel', 1), ('grim', 1), ('grou', 3), ('grow', 14), ('hafe', 1), ('hage', 1), ('hair', 1), ('hali', 1), ('hame', 1), ('hani', 1), ('hati', 1), ('havi', 1), ('hear', 5), ('heli', 1), ('hili', 1), ('hilo', 2), ('hiro', 1), ('hisp', 1), ('home', 2), ('hour', 4), ('howe', 10), ('ican', 1), ('ilin', 1), ('inve', 24), ('isel', 1), ('item', 2), ('jour', 8), ('juli', 2), ('juni', 1), ('juri', 1), ('lani', 1), ('lear', 5), ('lech', 1), ('lefe', 1), ('leri', 1), ('line', 3), ('lish', 4), ('lori', 5), ('lowe', 1), ('mani', 2), ('mari', 1), ('maru', 1), ('mile', 3), ('mili', 1), ('miro', 1), ('mist', 4), ('misy', 1), ('mith', 2), ('miti', 1), ('mone', 11), ('mono', 2), ('moro', 4), ('moun', 1), ('mune', 1), ('muni', 1), ('muno', 1), ('nati', 3), ('near', 7), ('nece', 3), ('nici', 2), ('nine', 1), ('niti', 1), ('nolo', 16), ('noun', 10), ('olen', 3), ('omen', 4), ('omur', 1), ('onis', 1), ('pear', 2), ('pelo', 1), ('peri', 16), ('peru', 1), ('plan', 4), ('plen', 1), ('plic', 8), ('plin', 1), ('plom', 2), ('plon', 1), ('polo', 1), ('poun', 4), ('prem', 1), ('prof', 1), ('prom', 1), ('prou', 1), ('rece', 15), ('reco', 5), ('rele', 13), ('reli', 1), ('remo', 7), ('rene', 1), ('rich', 1), ('rone', 1), ('roni', 1), ('roun', 8), ('ruci', 3), ('scan', 5), ('scel', 1), ('sche', 2), ('schi', 1), ('scho', 16), ('scon', 1), ('scor', 1), ('scre', 3), ('sear', 9), ('secu', 4), ('seni', 7), ('seri', 11), ('shab', 1), ('shar', 7), ('shaw', 2), ('shel', 5), ('shen', 1), ('shir', 2), ('shor', 4), ('shou', 1), ('show', 13), ('sici', 1), ('sili', 1), ('sine', 23), ('sist', 2), ('site', 1), ('siti', 6), ('situ', 2), ('sole', 1), ('soli', 3), ('some', 8), ('soni', 1), ('soun', 3), ('sour', 9), ('spar', 2), ('spec', 13), ('spel', 1), ('spen', 8), ('spon', 8), ('spor', 8), ('spou', 1), ('spre', 2), ('spro', 1), ('spur', 1), ('star', 24), ('stem', 5), ('ster', 6), ('stic', 4), ('stim', 5), ('stir', 1), ('stit', 1), ('stor', 10), ('stre', 15), ('stri', 9), ('stru', 5), ('stuc', 1), ('stun', 1), ('tech', 19), ('tele', 4), ('teni', 1), ('tere', 1), ('teri', 2), ('than', 5), ('them', 4), ('ther', 10), ('thic', 1), ('thin', 10), ('thir', 4), ('thom', 8), ('thou', 23), ('thre', 33), ('thur', 10), ('tici', 3), ('ticu', 1), ('timi', 1), ('tire', 1), ('titi', 2), ('titu', 1), ('tole', 1), ('toli', 1), ('tolo', 1), ('toni', 2), ('tori', 3), ('toru', 1), ('tour', 2), ('tran', 13), ('tren', 1), ('tric', 6), ('tril', 1), ('trol', 3), ('trou', 3), ('truc', 2), ('turi', 1), ('twor', 8), ('ulon', 1), ('unem', 3), ('uner', 1), ('velo', 16), ('vene', 1), ('vere', 1), ('veri', 2), ('vich', 1), ('visi', 2), ('viti', 2), ('wech', 1), ('whit', 2), ('with', 1), ('year', 1), ('youl', 1), ('youn', 13), ('200', 69), ('abe', 2), ('abo', 6), ('abu', 4), ('ace', 3), ('ach', 1), ('aci', 1), ('acu', 3), ('adi', 6), ('ado', 2), ('afe', 7), ('afi', 2), ('age', 3), ('agh', 9), ('agr', 1), ('agu', 9), ('aic', 2), ('ail', 12), ('aim', 3), ('ain', 25), ('air', 21), ('ais', 1), ('ake', 2), ('ale', 8), ('ali', 27), ('alo', 2), ('ame', 1), ('ami', 3), ('amu', 2), ('ane', 2), ('ani', 11), ('ano', 1), ('apu', 1), ('are', 15), ('ari', 9), ('aro', 2), ('asc', 3), ('ase', 2), ('ash', 4), ('asi', 6), ('asp', 1), ('ast', 3), ('ate', 17), ('ath', 12), ('ati', 23), ('ato', 2), ('atr', 1), ('atu', 2), ('auc', 1), ('aul', 6), ('aun', 6), ('aur', 3), ('avi', 11), ('aye', 2), ('bec', 25), ('bel', 4), ('ben', 11), ('ber', 13), ('bic', 2), ('bil', 14), ('bin', 1), ('bir', 8), ('bit', 3), ('bof', 1), ('bol', 2), ('bom', 10), ('bon', 6), ('bor', 8), ('bou', 1), ('bow', 1), ('buc', 6), ('bul', 4), ('bur', 11), ('can', 26), ('car', 37), ('cel', 5), ('cen', 41), ('cer', 20), ('cha', 3), ('che', 11), ('chi', 14), ('cho', 8), ('cil', 1), ('cin', 3), ('cir', 4), ('cit', 1), ('cle', 4), ('cli', 8), ('clo', 16), ('clu', 8), ('cof', 5), ('col', 15), ('com', 137), ('con', 166), ('cor', 27), ('cou', 2), ('cre', 36), ('cul', 5), ('cun', 2), ('cur', 21), ('dec', 2), ('def', 1), ('del', 14), ('den', 31), ('der', 33), ('dic', 23), ('dil', 5), ('din', 15), ('dir', 3), ('dis', 13), ('dol', 5), ('dom', 3), ('don', 1), ('dor', 1), ('dou', 7), ('dow', 19), ('duc', 26), ('ear', 30), ('ece', 1), ('ecu', 9), ('ele', 3), ('eli', 3), ('eni', 2), ('eno', 1), ('ero', 3), ('fec', 11), ('fel', 7), ('fem', 4), ('fen', 2), ('fer', 6), ('fic', 16), ('fil', 6), ('fin', 24), ('fir', 14), ('for', 104), ('fri', 24), ('gel', 1), ('gen', 10), ('ger', 12), ('ghe', 3), ('gho', 2), ('gic', 8), ('gin', 3), ('gir', 7), ('gis', 1), ('gol', 3), ('gon', 3), ('gor', 4), ('gre', 30), ('gri', 2), ('gro', 5), ('gru', 2), ('gul', 14), ('gun', 2), ('gur', 2), ('had', 2), ('hal', 6), ('ham', 9), ('han', 23), ('hap', 6), ('har', 10), ('has', 1), ('hat', 1), ('hau', 3), ('hav', 1), ('haw', 3), ('hay', 2), ('hec', 1), ('hel', 18), ('hem', 1), ('hen', 6), ('her', 4), ('hic', 2), ('hil', 3), ('him', 8), ('hin', 10), ('hir', 1), ('hol', 9), ('hom', 1), ('hon', 3), ('hor', 5), ('hou', 22), ('how', 1), ('huc', 2), ('hul', 1), ('hun', 8), ('hur', 3), ('ice', 1), ('ici', 4), ('ine', 4), ('ini', 1), ('ire', 2), ('iri', 2), ('isi', 1), ('iso', 2), ('ist', 5), ('isu', 1), ('jec', 26), ('jon', 3), ('jor', 1), ('jun', 2), ('jur', 2), ('kef', 1), ('ken', 4), ('ker', 2), ('lan', 24), ('lec', 12), ('lef', 9), ('lem', 8), ('len', 5), ('lex', 2), ('lic', 13), ('lim', 2), ('lin', 7), ('lit', 6), ('lof', 1), ('lom', 2), ('lon', 14), ('lor', 1), ('lou', 1), ('low', 11), ('man', 40), ('mar', 53), ('mel', 3), ('mem', 14), ('men', 29), ('mer', 12), ('mex', 3), ('mic', 3), ('mil', 4), ('min', 12), ('mir', 4), ('mis', 18), ('mit', 15), ('mol', 2), ('mon', 44), ('mor', 17), ('mou', 1), ('mul', 12), ('mur', 8), ('nec', 4), ('nel', 1), ('ner', 6), ('nex', 12), ('nic', 1), ('nil', 1), ('nin', 10), ('nis', 1), ('non', 10), ('nor', 21), ('nou', 1), ('now', 18), ('ole', 3), ('olo', 5), ('omo', 1), ('one', 3), ('oni', 1), ('ore', 9), ('ori', 2), ('oul', 2), ('oun', 18), ('our', 3), ('par', 67), ('pec', 16), ('pen', 28), ('peo', 3), ('per', 93), ('ple', 19), ('pli', 7), ('plo', 16), ('pol', 5), ('pon', 1), ('por', 36), ('pre', 43), ('pro', 146), ('pul', 10), ('pun', 1), ('pur', 12), ('ran', 36), ('rec', 7), ('ref', 11), ('rel', 17), ('rem', 11), ('ren', 10), ('ric', 8), ('ril', 1), ('rin', 13), ('ris', 3), ('rit', 5), ('rol', 8), ('rom', 2), ('ron', 6), ('ror', 3), ('rou', 27), ('row', 11), ('ruc', 1), ('run', 11), ('sch', 4), ('sci', 12), ('sco', 7), ('scu', 1), ('sec', 4), ('sef', 1), ('sel', 30), ('sem', 3), ('sen', 29), ('ser', 46), ('sex', 3), ('sha', 1), ('she', 3), ('shi', 26), ('sho', 16), ('shu', 1), ('sic', 14), ('sil', 2), ('sim', 5), ('sin', 25), ('sit', 2), ('sof', 5), ('sol', 11), ('som', 1), ('son', 8), ('sor', 7), ('sou', 13), ('spe', 8), ('spl', 1), ('spo', 15), ('ste', 24), ('sti', 23), ('sto', 13), ('str', 25), ('stu', 22), ('stw', 1), ('suc', 6), ('sul', 16), ('sun', 11), ('sur', 28), ('tec', 6), ('tel', 4), ('tem', 24), ('ten', 28), ('ter', 39), ('tex', 3), ('the', 27), ('thi', 3), ('tho', 19), ('thu', 1), ('tic', 14), ('tim', 6), ('tin', 25), ('tit', 7), ('tof', 1), ('tol', 2), ('tom', 5), ('ton', 8), ('tor', 21), ('tou', 3), ('tow', 11), ('tre', 11), ('tri', 34), ('tro', 16), ('tru', 3), ('tun', 1), ('tur', 32), ('twe', 16), ('uco', 1), ('ule', 1), ('uli', 2), ('uni', 24), ('uro', 10), ('vel', 3), ('vem', 5), ('ven', 28), ('ver', 80), ('vic', 11), ('vie', 24), ('vil', 11), ('vin', 6), ('vir', 1), ('vis', 4), ('vit', 2), ('wel', 17), ('wen', 4), ('whe', 7), ('who', 3), ('wor', 72), ('yer', 1), ('you', 7), ('00', 43), ('19', 32), ('ab', 36), ('ac', 160), ('ad', 83), ('af', 37), ('ag', 11), ('ai', 24), ('ak', 14), ('al', 202), ('am', 65), ('an', 204), ('ap', 98), ('ar', 176), ('as', 80), ('at', 60), ('au', 80), ('av', 33), ('aw', 26), ('ay', 35), ('be', 95), ('bi', 24), ('bo', 63), ('bu', 68), ('ce', 42), ('ch', 17), ('ci', 40), ('cl', 24), ('co', 44), ('cu', 49), ('de', 161), ('di', 97), ('do', 46), ('ef', 27), ('el', 9), ('em', 17), ('en', 132), ('er', 16), ('ex', 113), ('fe', 62), ('fi', 51), ('ge', 40), ('gh', 53), ('gi', 44), ('go', 63), ('gr', 35), ('gu', 36), ('ha', 3), ('he', 49), ('hi', 44), ('ho', 28), ('hu', 27), ('ic', 16), ('il', 45), ('im', 38), ('in', 209), ('ir', 25), ('is', 27), ('it', 37), ('jo', 35), ('ju', 49), ('ke', 60), ('le', 145), ('li', 137), ('lo', 123), ('me', 67), ('mi', 28), ('mo', 64), ('mu', 23), ('ne', 121), ('ni', 38), ('no', 39), ('of', 77), ('ol', 29), ('om', 5), ('on', 45), ('or', 61), ('ou', 26), ('ow', 26), ('pe', 46), ('pl', 58), ('po', 95), ('pu', 53), ('qu', 84), ('re', 300), ('ri', 121), ('ro', 105), ('ru', 42), ('sc', 15), ('se', 136), ('sh', 2), ('si', 125), ('so', 40), ('sp', 39), ('st', 145), ('su', 115), ('sy', 28), ('te', 62), ('th', 51), ('ti', 101), ('to', 90), ('tr', 95), ('tu', 26), ('tw', 7), ('uc', 3), ('ul', 21), ('un', 82), ('ur', 18), ('ve', 25), ('vi', 61), ('we', 61), ('wh', 10), ('ye', 12), ('0', 75), ('1', 188), ('2', 157), ('3', 94), ('4', 73), ('5', 84), ('6', 65), ('7', 79), ('8', 97), ('9', 77), ('_', 4), ('a', 112), ('b', 443), ('c', 325), ('d', 393), ('e', 374), ('f', 381), ('g', 245), ('h', 51), ('i', 156), ('j', 63), ('k', 215), ('l', 220), ('m', 291), ('n', 146), ('o', 284), ('p', 503), ('q', 38), ('r', 188), ('s', 392), ('t', 400), ('u', 238), ('v', 127), ('w', 289), ('x', 29), ('y', 189), ('z', 111), ('¼', 1), ('à', 1), ('á', 3), ('é', 4), ('ë', 1), ('ó', 1), ('ü', 3)]\n"
     ]
    }
   ],
   "source": [
    "training_iter_num = 299\n",
    "\n",
    "training_bpe_vocab = build_bpe_vocab(training_corpus)\n",
    "for i in range(training_iter_num):\n",
    "    \n",
    "    # 获取当前 BPE 词典中的 bigram 频次\n",
    "    bigram_freq = get_bigram_freq(training_bpe_vocab)\n",
    "    \n",
    "    # 找到频次最高的 bigram\n",
    "    if not bigram_freq:  # 若没有可合并的 bigram 则停止\n",
    "        break\n",
    "    most_frequent_bigram = max(bigram_freq, key=bigram_freq.get)\n",
    "    \n",
    "    # 合并该 bigram，得到更新后的 BPE 词典\n",
    "    training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(most_frequent_bigram, training_bpe_vocab)\n",
    "\n",
    "training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)\n",
    "print(training_bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "测试BPE分词器的分词效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0cfdb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturallanguageprocessing 的分词结果为：\n",
      "n atur al lan gu age pro ce s sing</w>\n"
     ]
    }
   ],
   "source": [
    "test_word = \"naturallanguageprocessing\"\n",
    "print(\"naturallanguageprocessing 的分词结果为：\")\n",
    "print_bpe_tokenize(test_word, training_bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5f2a1",
   "metadata": {},
   "source": [
    "### 实验总结\n",
    "> 在本次实验中，我们使用了字节对编码（Byte Pair Encoding, BPE）算法对英文句子进行分词处理。在示例中，我们的输入句子为 \"natural language processing\"，通过BPE算法处理后得到的分词结果为：n atur al lan gu age pro ce s sing</w>\\\n",
    "表明BPE算法能够有效地将较长的词汇拆分为多个子词，但是分割较为零碎，未能保留词汇的语义和结构。在该结果中，句中单词被正确识别为独立的亚词tokens。该分词结果较为有效的分离了信息，但是由于模型侧重，单词结构不能被有效识别和保留。\\\n",
    "通过本次实验，我们验证了BPE算法在英文分词任务中的有效性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
